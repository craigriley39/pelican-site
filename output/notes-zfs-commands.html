<!DOCTYPE html>
<html lang="en">
<head>
        <meta charset="utf-8" />
        <title>Notes - ZFS Commands</title>
        <link rel="stylesheet" href="http://justlearningdjango.com/theme/css/main.css" />

        <!--[if IE]>
            <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script>
        <![endif]-->
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="http://justlearningdjango.com/">Blog About Nothin. </a></h1>
                <nav><ul>
    
                        <li><a href="http://justlearningdjango.com/pages/about.html">About</a></li>
    
                        <li><a href="http://justlearningdjango.com/">Welcome to My Site</a></li>
                    <li><a href="http://justlearningdjango.com/category/blog.html">Blog</a></li>
                    <li><a href="http://justlearningdjango.com/category/linux.html">Linux</a></li>
                    <li class="active"><a href="http://justlearningdjango.com/category/unix.html">Unix</a></li>
                </ul>
<form id="search" action="/search.html" onsubmit="return (this.elements['q'].value.length > 0)">
		    <input id="searchbox" type="text" name="q" size="12" placeholder="Search">
		</form>
                </nav>
        </header><!-- /#banner -->
<section id="content" class="body">
  <article>
    <header>
      <h1 class="entry-title">
        <a href="http://justlearningdjango.com/notes-zfs-commands.html" rel="bookmark"
           title="Permalink to Notes - ZFS Commands">Notes - ZFS Commands</a></h1>
    </header>

    <div class="entry-content">
<footer class="post-info">
        <span>Mon 15 February 2016</span>
<span>| tags: <a href="http://justlearningdjango.com/tag/zfs.html">zfs</a><a href="http://justlearningdjango.com/tag/solaris.html">solaris</a></span>
</footer><!-- /.post-info -->      <h1>ZFS Basic commands</h1>
<div class="highlight"><pre><span></span><span class="c1">##</span>
<span class="c1">## External resources:</span>
http://www.solarisinternals.com/wiki/index.php/ZFS_Best_Practices_Guide

<span class="c1">##</span>
<span class="c1">## Create a file system in an existing pool:</span>

<span class="o">[</span>07:02:40 nxdev2<span class="o">]</span> ~ $ zpool list
NAME                    SIZE    USED   AVAIL    CAP  HEALTH     ALTROOT
tank                     68G   21.5G   46.5G    31%  ONLINE     -

<span class="o">[</span>07:02:43 nxdev2<span class="o">]</span> ~ $ zfs list
NAME                   USED  AVAIL  REFER  MOUNTPOINT
tank                  21.5G  45.4G  24.5K  /tank
tank/db01             3.28G  45.4G  3.28G  /db01
tank/twtc             18.2G  45.4G  28.5K  /twtc
tank/twtc/nxvsm       13.7G  45.4G  13.7G  /twtc/nxvsm
tank/twtc/nxvsmac     2.21G  45.4G  2.21G  /twtc/nxvsmac
tank/twtc/nxvsmgc     2.32G  45.4G  2.32G  /twtc/nxvsmgc

<span class="c1">### Create a new zfs file system - for /export/home/  500m quota on the fs</span>
sudo zfs create tank/exhome
sudo zfs create tank/exhome/awood
sudo zfs <span class="nb">set</span> <span class="nv">mountpoint</span><span class="o">=</span>/export/home tank/exhome
sudo zfs <span class="nb">set</span> <span class="nv">quota</span><span class="o">=</span>100m tank/exhome/awood
sudo zfs <span class="nb">set</span> <span class="nv">quota</span><span class="o">=</span>500m tank/exhome


<span class="o">[</span>07:13:24 nxdev2<span class="o">]</span> ~ $ zfs list
NAME                   USED  AVAIL  REFER  MOUNTPOINT
tank                  21.5G  45.4G  24.5K  /tank
tank/db01             3.28G  45.4G  3.28G  /db01
tank/exhome           51.5K   500M  25.5K  /export/home
tank/exhome/awood       26K   100M    26K  /export/home/awood
tank/twtc             18.2G  45.4G  28.5K  /twtc
tank/twtc/nxvsm       13.7G  45.4G  13.7G  /twtc/nxvsm
tank/twtc/nxvsmac     2.21G  45.4G  2.21G  /twtc/nxvsmac
tank/twtc/nxvsmgc     2.32G  45.4G  2.32G  /twtc/nxvsmgc

<span class="o">[</span>08:29:31 nxdev2<span class="o">]</span> share $ zfs list tank/exhome tank/exhome/awood
NAME                   USED  AVAIL  REFER  MOUNTPOINT
tank/exhome           51.5K   500M  25.5K  /export/home
tank/exhome/awood       26K   100M    26K  /export/home/awood


<span class="c1">#### Use zfs status to show the disks that make up a pool:</span>
<span class="o">[</span>08:30:54 nxdev2<span class="o">]</span> share $ zpool status tank
  pool: tank
 state: ONLINE
 scrub: scrub completed with <span class="m">0</span> errors on Mon Dec  <span class="m">3</span> 07:23:04 2007
config:

        NAME        STATE     READ WRITE CKSUM
        tank        ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
          mirror    ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
            c1t2d0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
            c1t3d0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0

errors: No known data errors


<span class="c1">## To initiate a block scrub to check for errors, run:</span>
sudo zpool scrub tank

<span class="c1">## Snapshots:</span>
    <span class="c1"># set up compression:</span>
    sudo zfs <span class="nb">set</span> <span class="nv">compression</span><span class="o">=</span>on tank/exhome

    <span class="c1"># show some of the filesystem attributes:</span>
    <span class="o">[</span>08:35:05 nxdev2<span class="o">]</span> share $ zfs get compression,quota  tank/exhome
    NAME             PROPERTY       VALUE                      SOURCE
    tank/exhome      compression    on                         <span class="nb">local</span>            
    tank/exhome      quota          500M                       <span class="nb">local</span>

    <span class="c1"># to show all, hey, use &#39;all&#39;  ---&gt;    zfs show all tank/exhome</span>

    <span class="c1"># create a snap shot, with just the changes today.</span>

    <span class="o">[</span>08:39:10 nxdev2<span class="o">]</span> share $ sudo zfs snapshot tank/exhome@snap1
    <span class="o">[</span>08:42:39 nxdev2<span class="o">]</span> share $ zfs list
    NAME                   USED  AVAIL  REFER  MOUNTPOINT
    tank                  21.5G  45.4G  24.5K  /tank
    tank/db01             3.28G  45.4G  3.28G  /db01
    tank/exhome           51.5K   500M  25.5K  /export/home
    tank/exhome@snap1         <span class="m">0</span>      -  25.5K  -
    tank/exhome/awood       26K   100M    26K  /export/home/awood
    tank/twtc             18.2G  45.4G  28.5K  /twtc
    tank/twtc/nxvsm       13.7G  45.4G  13.7G  /twtc/nxvsm
    tank/twtc/nxvsmac     2.21G  45.4G  2.21G  /twtc/nxvsmac
    tank/twtc/nxvsmgc     2.32G  45.4G  2.32G  /twtc/nxvsmgc

    <span class="o">[</span>08:42:41 nxdev2<span class="o">]</span> share $ ls -al /export/home/.
    ./  ../ 

    <span class="c1"># the snapshot is empty - no files were there</span>
</pre></div>


<h1>ZFS Root</h1>
<div class="highlight"><pre><span></span><span class="c1"># documenting steps to setup zfs root via jumpstart. </span>

<span class="c1">## TODO testing:</span>

play with boot -Z from PROM - list datasets
play with boot -L from PROM - list BE<span class="s1">&#39;s</span>
<span class="s1">boot alternate disk</span>
<span class="s1">    validate path with prtconf -vp |grep bootpath</span>

<span class="s1">test primary disk failure... this is a potential &#39;</span>con<span class="s1">&#39;.   if the primary mirror disk fails, </span>
<span class="s1">we&#39;</span>ll need to take downtime to repair.   Architecturally, this means maybe we should consider 
<span class="m">3</span> disk mirrors.   *Action - contact SUN to find out plan <span class="k">for</span> this not being an issue <span class="o">(</span>hot-swap 
of primary root disk<span class="o">)</span>

<span class="nb">test</span> adding a 3rd mirror-disk

play with root snapshots 
    - send to NFS or other server... 
    - roll back <span class="nb">local</span> <span class="p">;</span> roll back NFS
    - bare-metal restore to alt hardware <span class="o">(</span>DR<span class="o">)</span>

<span class="c1">## Considerations:</span>
-Any non-root zfs file systems we want need to be created early in the JS cycle, <span class="k">if</span> they are <span class="k">for</span> 
 post JS install <span class="o">(</span>such as creating /export/home early in a separate zpool<span class="o">)</span>

--Due to CR 6724860, you must run savecore manually to save a crash dump when using a
ZFS dump volume.  <span class="o">(</span>check with Conrad Geiger re: this<span class="o">)</span>



<span class="c1">## Here&#39;s the extent of the modifications to the profile (first run):</span>
    <span class="o">[</span>08:58:40 infraprdapp01<span class="o">]</span> PROFILES $ diff pststapp01.profile pststapp01.profile.orig_jumpstart 
    5,6c5,11
    &lt; pool rootpool auto auto auto mirror c1t0d0s0 c1t1d0s0
    &lt; bootenv installbe bename zfsroot dataset /var
    ---
    &gt; partitioning explicit
    &gt; filesys mirror:d10 c1t0d0s0 c1t1d0s0 <span class="m">10240</span> /
    &gt; filesys mirror:d20 c1t0d0s1 c1t1d0s1 <span class="m">32768</span> swap
    &gt; filesys mirror:d30 c1t0d0s3 c1t1d0s3 <span class="m">12288</span> /var
    &gt; filesys mirror:d40 c1t0d0s4 c1t1d0s4 <span class="m">12288</span> /export/home
    &gt; metadb c1t0d0s7 size <span class="m">8192</span> count 3
    &gt; metadb c1t1d0s7 size <span class="m">8192</span> count 3


<span class="c1">## The allocations above automatically partitioned a full slice 0 on the two disks.</span>
<span class="c1">## If additional devices are added later, they will prolly need to be explicitly partitioned</span>

<span class="c1">## By running zfs history, post-install, we can see the commands that were run to create the </span>
<span class="c1">## root zpool:</span>

    -bash-3.00# zpool <span class="nb">history</span>
    History <span class="k">for</span> <span class="s1">&#39;rootpool&#39;</span>:
    2009-07-09.07:54:43 zpool create -f -o <span class="nv">failmode</span><span class="o">=</span><span class="k">continue</span> -R /a -m legacy -o <span class="nv">cachefile</span><span class="o">=</span>/tmp/root/etc/zfs/zpool.cache rootpool mirror c1t0d0s0 c1t1d0s0
    2009-07-09.07:54:44 zfs <span class="nb">set</span> <span class="nv">canmount</span><span class="o">=</span>noauto rootpool
    2009-07-09.07:54:45 zfs <span class="nb">set</span> <span class="nv">mountpoint</span><span class="o">=</span>/rootpool rootpool
    2009-07-09.07:54:45 zfs create -o <span class="nv">mountpoint</span><span class="o">=</span>legacy rootpool/ROOT
    2009-07-09.07:54:46 zfs create -b <span class="m">8192</span> -V 2048m rootpool/swap
    2009-07-09.07:54:47 zfs create -b <span class="m">131072</span> -V 1536m rootpool/dump
    2009-07-09.07:55:06 zfs create -o <span class="nv">canmount</span><span class="o">=</span>noauto rootpool/ROOT/zfsroot
    2009-07-09.07:55:07 zfs create -o <span class="nv">canmount</span><span class="o">=</span>noauto rootpool/ROOT/zfsroot/var
    2009-07-09.07:55:08 zpool <span class="nb">set</span> <span class="nv">bootfs</span><span class="o">=</span>rootpool/ROOT/zfsroot rootpool
    2009-07-09.07:55:09 zfs <span class="nb">set</span> <span class="nv">mountpoint</span><span class="o">=</span>/ rootpool/ROOT/zfsroot
    2009-07-09.07:55:09 zfs <span class="nb">set</span> <span class="nv">canmount</span><span class="o">=</span>on rootpool
    2009-07-09.07:55:10 zfs create -o <span class="nv">mountpoint</span><span class="o">=</span>/export rootpool/export
    2009-07-09.07:55:11 zfs create rootpool/export/home


<span class="c1">## The ZFS pool version:</span>
    -bash-3.00# zpool upgrade
    This system is currently running ZFS pool version 10.

    All pools are formatted using this version.


<span class="c1">## Post install, here&#39;s what everything looked like:</span>

    -bash-3.00# zpool list  
    NAME       SIZE   USED  AVAIL    CAP  HEALTH  ALTROOT
    rootpool   136G  4.14G   132G     3%  ONLINE  -

    -bash-3.00# zpool status
      pool: rootpool
     state: ONLINE
     scrub: none requested
    config:

            NAME          STATE     READ WRITE CKSUM
            rootpool      ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
              mirror      ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
                c1t0d0s0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
                c1t1d0s0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0

    errors: No known data errors

    -bash-3.00# zfs list
    NAME                        USED  AVAIL  REFER  MOUNTPOINT
    rootpool                   6.14G   128G    94K  /rootpool
    rootpool/ROOT              2.63G   128G    18K  legacy
    rootpool/ROOT/zfsroot      2.63G   128G  2.43G  /
    rootpool/ROOT/zfsroot/var   214M   128G   214M  /var
    rootpool/dump              1.50G   128G  1.50G  -
    rootpool/export              38K   128G    20K  /export
    rootpool/export/home         18K   128G    18K  /export/home
    rootpool/swap                 2G   130G    16K  -

    <span class="c1">## Note that the default sizing for swap and dump sized them at 2G and 1.5G respectively:</span>
    pool rootpool auto auto auto mirror c1t0d0s0 c1t1d0s0
    <span class="o">(</span>It<span class="s1">&#39;s the 2nd and 3rd &quot;auto&#39;</span>s<span class="s2">&quot; ) that size them</span>



<span class="s2">    -bash-3.00# swap -l</span>
<span class="s2">    swapfile             dev  swaplo blocks   free</span>
<span class="s2">    /dev/zvol/dsk/rootpool/swap 256,2      16 4194288 4194288</span>

<span class="s2">    -bash-3.00# dumpadm</span>
<span class="s2">          Dump content: kernel pages</span>
<span class="s2">           Dump device: /dev/zvol/dsk/rootpool/dump (dedicated)</span>
<span class="s2">    Savecore directory: /var/crash/pststapp01</span>
<span class="s2">      Savecore enabled: yes</span>


<span class="s2">    ## The default settings for the root filesystem:</span>

<span class="s2">    -- The BE info:</span>
<span class="s2">    ---------------</span>
<span class="s2">    -bash-3.00# lustatus</span>
<span class="s2">    Boot Environment           Is       Active Active    Can    Copy      </span>
<span class="s2">    Name                       Complete Now    On Reboot Delete Status    </span>
<span class="s2">    -------------------------- -------- ------ --------- ------ ----------</span>
<span class="s2">    zfsroot                    yes      yes    yes       no     -         </span>
<span class="s2">    -bash-3.00# lucurr</span>
<span class="s2">    zfsroot</span>

<span class="s2">    -- The zfs settings for root for that BE:</span>
<span class="s2">    command notes: </span>
<span class="s2">        zfs get -r (recursive) &quot;</span>all<span class="s2">&quot;|property[,...] fs|vol|snapshot</span>

<span class="s2">    -bash-3.00# zfs get -r all  rootpool/ROOT/zfsroot</span>
<span class="s2">    NAME                       PROPERTY         VALUE                  SOURCE</span>
<span class="s2">    rootpool/ROOT/zfsroot      type             filesystem             -</span>
<span class="s2">    rootpool/ROOT/zfsroot      creation         Thu Jul  9  7:55 2009  -</span>
<span class="s2">    rootpool/ROOT/zfsroot      used             2.64G                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot      available        128G                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot      referenced       2.43G                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot      compressratio    1.00x                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot      mounted          yes                    -</span>
<span class="s2">    rootpool/ROOT/zfsroot      quota            none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot      reservation      none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot      recordsize       128K                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot      mountpoint       /                      local</span>
<span class="s2">    rootpool/ROOT/zfsroot      sharenfs         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      checksum         on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      compression      off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      atime            on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      devices          on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      exec             on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      setuid           on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      readonly         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      zoned            off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      snapdir          hidden                 default</span>
<span class="s2">    rootpool/ROOT/zfsroot      aclmode          groupmask              default</span>
<span class="s2">    rootpool/ROOT/zfsroot      aclinherit       restricted             default</span>
<span class="s2">    rootpool/ROOT/zfsroot      canmount         noauto                 local</span>
<span class="s2">    rootpool/ROOT/zfsroot      shareiscsi       off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      xattr            on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot      copies           1                      default</span>
<span class="s2">    rootpool/ROOT/zfsroot      version          3                      -</span>
<span class="s2">    rootpool/ROOT/zfsroot      utf8only         off                    -</span>
<span class="s2">    rootpool/ROOT/zfsroot      normalization    none                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot      casesensitivity  sensitive              -</span>
<span class="s2">    rootpool/ROOT/zfsroot      vscan            off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      nbmand           off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      sharesmb         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot      refquota         none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot      refreservation   none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  type             filesystem             -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  creation         Thu Jul  9  7:55 2009  -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  used             215M                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  available        128G                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  referenced       215M                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  compressratio    1.00x                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  mounted          yes                    -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  quota            none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  reservation      none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  recordsize       128K                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  mountpoint       /var                   inherited from rootpool/ROOT/zfsroot</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  sharenfs         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  checksum         on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  compression      off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  atime            on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  devices          on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  exec             on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  setuid           on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  readonly         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  zoned            off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  snapdir          hidden                 default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  aclmode          groupmask              default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  aclinherit       restricted             default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  canmount         noauto                 local</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  shareiscsi       off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  xattr            on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  copies           1                      default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  version          3                      -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  utf8only         off                    -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  normalization    none                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  casesensitivity  sensitive              -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  vscan            off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  nbmand           off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  sharesmb         off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  refquota         none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  refreservation   none                   default</span>


<span class="s2">    ## Items of note:</span>
<span class="s2">    no compression:</span>
<span class="s2">    rootpool/ROOT/zfsroot      compression      off                    default</span>

<span class="s2">    no sharenfs:</span>
<span class="s2">    rootpool/ROOT/zfsroot      sharenfs         off                    default</span>

<span class="s2">    checksum is on:</span>
<span class="s2">    rootpool/ROOT/zfsroot      checksum         on                     default</span>

<span class="s2">    no size restrictions:</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  quota            none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  reservation      none                   default</span>

<span class="s2">    rootpool/ROOT/zfsroot/var  refquota         none                   default</span>
<span class="s2">    rootpool/ROOT/zfsroot/var  refreservation   none                   default</span>

<span class="s2">## And the disk partitioning:</span>
<span class="s2">    -bash-3.00# prtvtoc /dev/rdsk/c1t0d0s0</span>
<span class="s2">    * /dev/rdsk/c1t0d0s0 partition map</span>
<span class="s2">    *</span>
<span class="s2">    * Dimensions:</span>
<span class="s2">    *     512 bytes/sector</span>
<span class="s2">    *     848 sectors/track</span>
<span class="s2">    *      24 tracks/cylinder</span>
<span class="s2">    *   20352 sectors/cylinder</span>
<span class="s2">    *   14089 cylinders</span>
<span class="s2">    *   14087 accessible cylinders</span>
<span class="s2">    *</span>
<span class="s2">    * Flags:</span>
<span class="s2">    *   1: unmountable</span>
<span class="s2">    *  10: read-only</span>
<span class="s2">    *</span>
<span class="s2">    *                          First     Sector    Last</span>
<span class="s2">    * Partition  Tag  Flags    Sector     Count    Sector  Mount Directory</span>
<span class="s2">           0      2    00          0 286698624 286698623</span>
<span class="s2">           2      5    00          0 286698624 286698623</span>


<span class="s2">    -bash-3.00# prtvtoc /dev/rdsk/c1t1d0s0</span>
<span class="s2">    * /dev/rdsk/c1t1d0s0 partition map</span>
<span class="s2">    *</span>
<span class="s2">    * Dimensions:</span>
<span class="s2">    *     512 bytes/sector</span>
<span class="s2">    *     848 sectors/track</span>
<span class="s2">    *      24 tracks/cylinder</span>
<span class="s2">    *   20352 sectors/cylinder</span>
<span class="s2">    *   14089 cylinders</span>
<span class="s2">    *   14087 accessible cylinders</span>
<span class="s2">    *</span>
<span class="s2">    * Flags:</span>
<span class="s2">    *   1: unmountable</span>
<span class="s2">    *  10: read-only</span>
<span class="s2">    *</span>
<span class="s2">    *                          First     Sector    Last</span>
<span class="s2">    * Partition  Tag  Flags    Sector     Count    Sector  Mount Directory</span>
<span class="s2">           0      0    00          0 286698624 286698623</span>
<span class="s2">           2      5    00          0 286698624 286698623</span>


<span class="s2">### Now, let&#39;s setup some resource limitations:</span>

<span class="s2">10GB quota for /var:</span>
<span class="s2">    -bash-3.00# zfs set quota=10g rootpool/ROOT/zfsroot/var</span>

<span class="s2">    -bash-3.00# zfs list -o name,used,avail,refer,quota rootpool/ROOT/zfsroot/var</span>
<span class="s2">    NAME                        USED  AVAIL  REFER  QUOTA</span>
<span class="s2">    rootpool/ROOT/zfsroot/var   283M  9.72G   283M    10G</span>

<span class="s2">    -bash-3.00# df -h</span>
<span class="s2">    Filesystem             size   used  avail capacity  Mounted on</span>
<span class="s2">    rootpool/ROOT/zfsroot</span>
<span class="s2">                           134G   2.4G   128G     2%    /</span>
<span class="s2">    /devices                 0K     0K     0K     0%    /devices</span>
<span class="s2">    ctfs                     0K     0K     0K     0%    /system/contract</span>
<span class="s2">    proc                     0K     0K     0K     0%    /proc</span>
<span class="s2">    mnttab                   0K     0K     0K     0%    /etc/mnttab</span>
<span class="s2">    swap                    28G   1.2M    28G     1%    /etc/svc/volatile</span>
<span class="s2">    objfs                    0K     0K     0K     0%    /system/object</span>
<span class="s2">    sharefs                  0K     0K     0K     0%    /etc/dfs/sharetab</span>
<span class="s2">    /platform/SUNW,SPARC-Enterprise-T5220/lib/libc_psr/libc_psr_hwcap2.so.1</span>
<span class="s2">                           130G   2.4G   128G     2%    /platform/sun4v/lib/libc_psr.so.1</span>
<span class="s2">    /platform/SUNW,SPARC-Enterprise-T5220/lib/sparcv9/libc_psr/libc_psr_hwcap2.so.1</span>
<span class="s2">                           130G   2.4G   128G     2%    /platform/sun4v/lib/sparcv9/libc_psr.so.1</span>
<span class="s2">    fd                       0K     0K     0K     0%    /dev/fd</span>
<span class="s2">    rootpool/ROOT/zfsroot/var</span>
<span class="s2">                            10G   283M   9.7G     3%    /var    &lt;-------------------</span>
<span class="s2">    swap                    28G    96K    28G     1%    /tmp</span>
<span class="s2">    swap                    28G    32K    28G     1%    /var/run</span>
<span class="s2">    rootpool/export        134G    20K   128G     1%    /export</span>
<span class="s2">    rootpool/export/home   134G    18K   128G     1%    /export/home</span>
<span class="s2">    rootpool               134G    94K   128G     1%    /rootpool</span>


<span class="s2">10G reserved for / (not including snaps)</span>

<span class="s2">    -bash-3.00# df -h /</span>
<span class="s2">    Filesystem             size   used  avail capacity  Mounted on</span>
<span class="s2">    rootpool/ROOT/zfsroot</span>
<span class="s2">                           134G   2.4G   128G     2%    /</span>

<span class="s2">    -bash-3.00# zfs list /</span>
<span class="s2">    NAME                    USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/ROOT/zfsroot  2.70G   128G  2.43G  /</span>

<span class="s2">    -bash-3.00# zfs set refreservation=10g rootpool/ROOT/zfsroot</span>
<span class="s2">    -bash-3.00# zfs list /</span>
<span class="s2">    NAME                    USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/ROOT/zfsroot  10.3G   128G  2.43G  /</span>


<span class="s2">## Now, let&#39;s take a snapshot of / only</span>
<span class="s2">    -bash-3.00# zfs snapshot rootpool/ROOT/zfsroot@20090714a</span>
<span class="s2">    -bash-3.00# zfs list -r /</span>
<span class="s2">    NAME                              USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/ROOT/zfsroot            12.7G   128G  2.43G  /</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a      0      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var         283M  9.72G   283M  /var</span>

<span class="s2">    -bash-3.00# zfs get all rootpool/ROOT/zfsroot@20090714a</span>
<span class="s2">    NAME                             PROPERTY         VALUE                  SOURCE</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  type             snapshot               -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  creation         Tue Jul 14 22:04 2009  -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  used             0                      -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  referenced       2.43G                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  compressratio    1.00x                  -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  devices          on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  exec             on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  setuid           on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  shareiscsi       off                    default</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  xattr            on                     default</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  version          3                      -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  utf8only         off                    -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  normalization    none                   -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  casesensitivity  sensitive              -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  nbmand           off                    default</span>

<span class="s2">    -bash-3.00# ls -al /.zfs</span>
<span class="s2">    total 5</span>
<span class="s2">    dr-xr-xr-x   3 root     root           3 Jul  9 07:55 .</span>
<span class="s2">    drwxr-xr-x  26 root     root          29 Jul 14 14:19 ..</span>
<span class="s2">    dr-xr-xr-x   2 root     root           2 Jul  9 07:55 snapshot</span>

<span class="s2">## Recover a file:</span>
<span class="s2">    -bash-3.00# cat /etc/motd</span>
<span class="s2">    Sun Microsystems Inc.   SunOS 5.10      Generic January 2005</span>
<span class="s2">    -bash-3.00# echo &#39;mike o is da bomb!&#39; &gt; /etc/motd</span>
<span class="s2">    -bash-3.00# cat /etc/motd</span>
<span class="s2">    mike o is da bomb!</span>
<span class="s2">    -bash-3.00# cp /.zfs/snapshot/20090714a/etc/motd /etc/motd</span>
<span class="s2">    -bash-3.00# cat /etc/motd</span>
<span class="s2">    Sun Microsystems Inc.   SunOS 5.10      Generic January 2005</span>

<span class="s2">## and, now that the dataset changed (regardless of the data returning to original data), </span>
<span class="s2">## the size of the snapshot has changed:</span>
<span class="s2">    -bash-3.00# zfs list -r /</span>
<span class="s2">    NAME                              USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/ROOT/zfsroot            12.7G   128G  2.43G  /</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a  38.5K      -  2.43G  -    &lt;----------</span>
<span class="s2">    rootpool/ROOT/zfsroot/var         283M  9.72G   283M  /var</span>

<span class="s2">##  Now, let&#39;s create a recursive snapshot of /, which will include it&#39;s dependencies:</span>
<span class="s2">    -bash-3.00# zfs snapshot -r rootpool/ROOT/zfsroot@20090714b</span>
<span class="s2">    -bash-3.00# zfs list -r /</span>
<span class="s2">    NAME                                  USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/ROOT/zfsroot                12.7G   128G  2.43G  /</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a      38.5K      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714b          0      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/zfsroot/var             283M  9.72G   283M  /var</span>
<span class="s2">    rootpool/ROOT/zfsroot/var@20090714b      0      -   283M  -</span>

<span class="s2">    # note that /var and / zfs filesystems both have a snapshot named the same.</span>

<span class="s2">## Here&#39;s all the snapshots:</span>
<span class="s2">    -bash-3.00# zfs list -t snapshot -r -o name,creation /</span>
<span class="s2">    NAME                                 CREATION</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714a      Tue Jul 14 22:04 2009</span>
<span class="s2">    rootpool/ROOT/zfsroot@20090714b      Tue Jul 14 22:14 2009</span>
<span class="s2">    rootpool/ROOT/zfsroot/var@20090714b  Tue Jul 14 22:14 2009</span>


<span class="s2">## dump and swap:</span>
<span class="s2">    -bash-3.00# dumpadm      </span>
<span class="s2">          Dump content: kernel pages</span>
<span class="s2">           Dump device: /dev/zvol/dsk/rootpool/dump (dedicated)</span>
<span class="s2">    Savecore directory: /var/crash/pststapp01</span>
<span class="s2">      Savecore enabled: yes</span>

<span class="s2">    -bash-3.00# swap -l</span>
<span class="s2">    swapfile             dev  swaplo blocks   free</span>
<span class="s2">    /dev/zvol/dsk/rootpool/swap 256,2      16 4194288 4194288</span>


<span class="s2">    ## and the devices:</span>
<span class="s2">    -bash-3.00# ls -al   /dev/zvol/dsk/rootpool/</span>
<span class="s2">    total 8</span>
<span class="s2">    drwxr-xr-x   2 root     root           4 Jul  9 08:19 .</span>
<span class="s2">    drwxr-xr-x   3 root     root           3 Jul  9 08:08 ..</span>
<span class="s2">    lrwxrwxrwx   1 root     root          35 Jul  9 08:19 dump -&gt; ../../../../devices/pseudo/zfs@0:1c</span>
<span class="s2">    lrwxrwxrwx   1 root     root          35 Jul  9 08:19 swap -&gt; ../../../../devices/pseudo/zfs@0:2c</span>

<span class="s2">    -bash-3.00# zfs get all rootpool/dump rootpool/swap</span>
<span class="s2">    NAME           PROPERTY         VALUE                  SOURCE</span>
<span class="s2">    rootpool/dump  type             volume                 -</span>
<span class="s2">    rootpool/dump  creation         Thu Jul  9  7:54 2009  -</span>
<span class="s2">    rootpool/dump  used             1.50G                  -</span>
<span class="s2">    rootpool/dump  available        118G                   -</span>
<span class="s2">    rootpool/dump  referenced       1.50G                  -</span>
<span class="s2">    rootpool/dump  compressratio    1.00x                  -</span>
<span class="s2">    rootpool/dump  reservation      none                   default</span>
<span class="s2">    rootpool/dump  volsize          1.50G                  -</span>
<span class="s2">    rootpool/dump  volblocksize     128K                   -</span>
<span class="s2">    rootpool/dump  checksum         off                    local</span>
<span class="s2">    rootpool/dump  compression      off                    local</span>
<span class="s2">    rootpool/dump  readonly         off                    default</span>
<span class="s2">    rootpool/dump  shareiscsi       off                    default</span>
<span class="s2">    rootpool/dump  copies           1                      default</span>
<span class="s2">    rootpool/dump  refreservation   none                   default</span>
<span class="s2">    rootpool/swap  type             volume                 -</span>
<span class="s2">    rootpool/swap  creation         Thu Jul  9  7:54 2009  -</span>
<span class="s2">    rootpool/swap  used             2G                     -</span>
<span class="s2">    rootpool/swap  available        120G                   -</span>
<span class="s2">    rootpool/swap  referenced       16K                    -</span>
<span class="s2">    rootpool/swap  compressratio    1.00x                  -</span>
<span class="s2">    rootpool/swap  reservation      none                   default</span>
<span class="s2">    rootpool/swap  volsize          2G                     -</span>
<span class="s2">    rootpool/swap  volblocksize     8K                     -</span>
<span class="s2">    rootpool/swap  checksum         on                     default</span>
<span class="s2">    rootpool/swap  compression      off                    default</span>
<span class="s2">    rootpool/swap  readonly         off                    default</span>
<span class="s2">    rootpool/swap  shareiscsi       off                    default</span>
<span class="s2">    rootpool/swap  copies           1                      default</span>
<span class="s2">    rootpool/swap  refreservation   2G                     local</span>

<span class="s2">## Resize dump device and swap to 4g each</span>

<span class="s2">    #  Resize Dump</span>
<span class="s2">    -bash-3.00# zfs list rootpool/dump</span>
<span class="s2">    NAME            USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/dump  1.50G   118G  1.50G  -</span>

<span class="s2">    -bash-3.00# zfs get volsize  rootpool/dump</span>
<span class="s2">    NAME           PROPERTY  VALUE          SOURCE</span>
<span class="s2">    rootpool/dump  volsize   1.50G          -</span>

<span class="s2">    -bash-3.00# zfs set volsize=4g  rootpool/dump</span>

<span class="s2">    -bash-3.00# zfs get volsize  rootpool/dump</span>
<span class="s2">    NAME           PROPERTY  VALUE          SOURCE</span>
<span class="s2">    rootpool/dump  volsize   4G             -</span>

<span class="s2">    -bash-3.00# zfs list rootpool/dump </span>
<span class="s2">    NAME            USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/dump  4.00G   115G  4.00G  -</span>


<span class="s2">    # resize swap:</span>
<span class="s2">    -bash-3.00# zfs list rootpool/swap </span>
<span class="s2">    NAME            USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool/swap     2G   117G    16K  -</span>

<span class="s2">    -bash-3.00# swap -l               </span>
<span class="s2">    swapfile             dev  swaplo blocks   free</span>
<span class="s2">    /dev/zvol/dsk/rootpool/swap 256,2      16 4194288 4194288</span>

<span class="s2">    # remove the swap space</span>
<span class="s2">    -bash-3.00# swap -d /dev/zvol/dsk/rootpool/swap</span>

<span class="s2">    -bash-3.00# swap -l</span>
<span class="s2">    No swap devices configured</span>

<span class="s2">    # grow the swap zfs volume</span>
<span class="s2">    -bash-3.00# zfs set volsize=4g rootpool/swap</span>

<span class="s2">    -bash-3.00# zfs get volsize rootpool/swap</span>
<span class="s2">    NAME           PROPERTY  VALUE          SOURCE</span>
<span class="s2">    rootpool/swap  volsize   4G             -</span>

<span class="s2">    # re-add the swap device</span>
<span class="s2">    -bash-3.00# swap -a /dev/zvol/dsk/rootpool/swap</span>
<span class="s2">    /dev/zvol/dsk/rootpool/swap is in use for live upgrade -. Please see ludelete(1M).</span>

<span class="s2">    # so, use swapadd instead</span>
<span class="s2">    -bash-3.00# /sbin/swapadd</span>
<span class="s2">    -bash-3.00# swap -l</span>
<span class="s2">    swapfile             dev  swaplo blocks   free</span>
<span class="s2">    /dev/zvol/dsk/rootpool/swap 256,2      16 8388592 8388592</span>
</pre></div>


<h1>ZFS Boot Environment and Live Upgrade</h1>
<div class="highlight"><pre><span></span><span class="c1">## Tasks:</span>
create a second BE <span class="o">(</span>lucreate<span class="o">)</span>
patch the second BE with latest patches <span class="o">(</span>luupgrade<span class="o">)</span>
make the 2nd BE bootable and active on next reboot <span class="o">(</span>luactivate<span class="o">)</span>
compare the changed files across the BE<span class="s1">&#39;s (lucompare)</span>
<span class="s1">mount file systems from the inactive BE (lumount) and umount (luumount)</span>
<span class="s1">boot from the new BE (test boot -Z display)</span>
<span class="s1">boot again from the 1st BE (simulate roll-back) (luactivate)</span>


<span class="s1">## Coniderations:</span>

<span class="s1"> Live Upgrade Issues</span>

<span class="s1">    * The Solaris installation GUI&#39;</span>s standard-upgrade option is not available <span class="k">for</span> migrating from a UFS to a ZFS root 
        file system. To migrate from a UFS file system, you must use Solaris Live Upgrade.
    * You cannot use Solaris Live Upgrade to create a UFS BE from a ZFS BE.
    * Do not rename your ZFS BEs with the zfs rename <span class="nb">command</span> because the Solaris Live Upgrade feature is unaware of 
        the name change. Subsequent commands, such as ludelete, will fail. In fact, <span class="k">do</span> not rename your ZFS pools or 
        file systems <span class="k">if</span> you have existing BEs that you want to <span class="k">continue</span> to use.
    * Solaris Live Upgrade creates the datasets <span class="k">for</span> the BE and ZFS volumes <span class="k">for</span> the swap area and dump device but does 
        not account <span class="k">for</span> any existing dataset property modifications. Thus, <span class="k">if</span> you want a dataset property enabled in 
        the new BE, you must <span class="nb">set</span> the property before the lucreate operation. For example: 

         zfs <span class="nb">set</span> <span class="nv">compression</span><span class="o">=</span>on rpool/ROOT

    * When creating an alternative BE that is a clone of the primary BE, you cannot use the -f, -x, -y, -Y, and -z 
        options to include or exclude files from the primary BE. You can still use the inclusion and exclusion option 
        <span class="nb">set</span> in the following cases: 

        UFS -&gt; UFS UFS -&gt; ZFS ZFS -&gt; ZFS <span class="o">(</span>different pool<span class="o">)</span>

    * Although you can use Solaris Live Upgrade to upgrade your UFS root file system to a ZFS root file system, you 
            cannot use Solaris Live Upgrade to upgrade non-root or shared file systems.
    * On a SPARC system that runs the Solaris <span class="m">10</span> 5/09 release, <span class="nb">set</span> the BOOT_MENU_FILE variable before activating the 
        ZFS BE with luactivate, due to CR 6824589. 

        <span class="c1"># BOOT_MENU_FILE=&quot;menu.lst&quot;</span>
        <span class="c1"># export BOOT_MENU_FILE</span>


<span class="c1">## starting zfs environment:</span>

    -bash-3.00# zfs list
    NAME                                  USED  AVAIL  REFER  MOUNTPOINT
    rootpool                             20.8G   113G    94K  /rootpool
    rootpool/ROOT                        12.8G   113G    18K  legacy
    rootpool/ROOT/zfsroot                12.8G   123G  2.43G  /
    rootpool/ROOT/zfsroot@20090714a      38.5K      -  2.43G  -
    rootpool/ROOT/zfsroot@20090714b        38K      -  2.43G  -
    rootpool/ROOT/zfsroot/var             362M  9.65G   361M  /var
    rootpool/ROOT/zfsroot/var@20090714b  1.63M      -   283M  -
    rootpool/dump                        4.00G   113G  4.00G  -
    rootpool/export                        38K   113G    20K  /export
    rootpool/export/home                   18K   113G    18K  /export/home
    rootpool/swap                           4G   117G    16K  -


    -bash-3.00# zpool status
      pool: rootpool
     state: ONLINE
     scrub: none requested
    config:

            NAME          STATE     READ WRITE CKSUM
            rootpool      ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
              mirror      ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
                c1t0d0s0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0
                c1t1d0s0  ONLINE       <span class="m">0</span>     <span class="m">0</span>     0

    errors: No known data errors

<span class="c1">## Starting BE setup:</span>
    -bash-3.00# lustatus
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    yes       no     -       


    -bash-3.00# cat /etc/lutab
    <span class="c1"># DO NOT EDIT THIS FILE BY HAND. This file is not a public interface.</span>
    <span class="c1"># The format and contents of this file are subject to change.</span>
    <span class="c1"># Any user modification to this file may result in the incorrect</span>
    <span class="c1"># operation of Live Upgrade.</span>
    1:zfsroot:C:0
    1:/:rootpool/ROOT/zfsroot:1
    1:boot-device:/dev/dsk/c1t0d0s0:2

    -bash-3.00# lucurr
    zfsroot

<span class="c1">## From infodoc 206844, minimum sol10 LU patch levels:</span>

    The following patches provide Live Upgrade functionality <span class="k">for</span> Solaris <span class="m">10</span> SPARC:
    If you are running non-global zones, see the subsequent section <span class="k">for</span> additional patches which you must also install.
    118815-05 or higher nawk patch
    120900-04 or higher libzonecfg patch
    121133-02 or higher SUNWzoneu required patch
    119254-64 or higher Install and Patch Utilities Patch**
    119317-01 or higher SVr4 Packaging Commands <span class="o">(</span>usr<span class="o">)</span> patch
    120235-01 or higher SUNWluzone required patches
    121428-08 or higher SUNWluzone required patches
    121002-03 or higher pax patches
    123121-02 or higher prodreg patches
    The following patch is only required <span class="k">if</span> the TSIpgx package is installed:
    119309-03 or higher PGX32 Graphics <span class="o">(</span>TSIpgx Power Management<span class="o">)</span>
    121004-03 or higher sh patch
    119574-02 or higher su patch
    120996-02 or higher cpio patch
    120068-03 or higher telnet security patch
    119042-10 or higher /usr/sbin/svccfg patch
    126538-01 or higher i.manifest r.manifest class action script patch
    123332-01 or higher tftp patch
    119246-27 or higher Man pages patch
    121901-02 or higher i.manifest r.manifest class action script patch
    125418-01 or higher in.telnetd patch
    121430-34 or higher Live Upgrade patch
    123839-07 or higher Fault Manager patch
    127922-03 or higher cpio patch
    137321-01 or higher p7zip patch <span class="o">(</span>required <span class="k">if</span> upgrading to Solaris <span class="m">10</span> 5/08 or higher<span class="o">)</span>
    138130-01 or higher vold patch
    If applied to the live boot environment, the system should be rebooted once all patches are applied.

    <span class="c1">## All levels validated to exceed those above.</span>


    <span class="c1">## But, install the latest patch for Live Upgrade (best practice)</span>
    <span class="c1">## This particular patch addresses the luactivate issue (the work-around of exporting BOOT_MENU_FILE)</span>

    -bash-3.00# patchadd -d 121430-37
    Validating patches...

    Loading patches installed on the system...
    Done!
    Loading patches requested to install.
    Done!
    Checking patches that you specified <span class="k">for</span> installation.
    Done!
    Approved patches will be installed in this order:
    121430-37 
    Checking installed patches...
    Executing prepatch script...
    Verifying sufficient filesystem capacity <span class="o">(</span>dry run method<span class="o">)</span>...
    Installing patch packages...

    Patch 121430-37 has been successfully installed.
    See /var/sadm/patch/121430-37/log <span class="k">for</span> details

    Patch packages installed:
      SUNWlucfg
      SUNWlur
      SUNWluu


<span class="c1">## Now, we can start:</span>


<span class="c1">## Create the second BE:</span>
    -bash-3.00# lucreate -n 20090721mao
    Analyzing system configuration.
    Comparing <span class="nb">source</span> boot environment &lt;zfsroot&gt; file systems with the file 
    system<span class="o">(</span>s<span class="o">)</span> you specified <span class="k">for</span> the new boot environment. Determining which 
    file systems should be in the new boot environment.
    Updating boot environment description database on all BEs.
    Updating system configuration files.
    Creating configuration <span class="k">for</span> boot environment &lt;20090721mao&gt;.
    Source boot environment is &lt;zfsroot&gt;.
    Creating boot environment &lt;20090721mao&gt;.
    Cloning file systems from boot environment &lt;zfsroot&gt; to create boot environment &lt;20090721mao&gt;.
    Creating snapshot <span class="k">for</span> &lt;rootpool/ROOT/zfsroot&gt; on &lt;rootpool/ROOT/zfsroot@20090721mao&gt;.
    Creating clone <span class="k">for</span> &lt;rootpool/ROOT/zfsroot@20090721mao&gt; on &lt;rootpool/ROOT/20090721mao&gt;.
    Setting <span class="nv">canmount</span><span class="o">=</span>noauto <span class="k">for</span> &lt;/&gt; in zone &lt;global&gt; on &lt;rootpool/ROOT/20090721mao&gt;.
    Creating snapshot <span class="k">for</span> &lt;rootpool/ROOT/zfsroot/var&gt; on &lt;rootpool/ROOT/zfsroot/var@20090721mao&gt;.
    Creating clone <span class="k">for</span> &lt;rootpool/ROOT/zfsroot/var@20090721mao&gt; on &lt;rootpool/ROOT/20090721mao/var&gt;.
    Setting <span class="nv">canmount</span><span class="o">=</span>noauto <span class="k">for</span> &lt;/var&gt; in zone &lt;global&gt; on &lt;rootpool/ROOT/20090721mao/var&gt;.
    Population of boot environment &lt;20090721mao&gt; successful.
    Creation of boot environment &lt;20090721mao&gt; successful.


    -bash-3.00# zfs list
    NAME                                    USED  AVAIL  REFER  MOUNTPOINT
    rootpool                               24.5G   109G    94K  /rootpool
    rootpool/ROOT                          16.5G   109G    18K  legacy
    rootpool/ROOT/20090721mao               145K   109G  2.43G  /               &lt;-----
    rootpool/ROOT/20090721mao/var            50K   109G  4.06G  /var            &lt;-----
    rootpool/ROOT/zfsroot                  16.5G   119G  2.43G  /
    rootpool/ROOT/zfsroot@20090714a        38.5K      -  2.43G  -
    rootpool/ROOT/zfsroot@20090714b          38K      -  2.43G  -
    rootpool/ROOT/zfsroot@20090721mao      69.5K      -  2.43G  -
    rootpool/ROOT/zfsroot/var              4.07G  5.93G  4.06G  /var
    rootpool/ROOT/zfsroot/var@20090714b    10.4M      -   283M  -
    rootpool/ROOT/zfsroot/var@20090721mao   310K      -  4.06G  -               &lt;-----
    rootpool/dump                          4.00G   109G  4.00G  -
    rootpool/export                          38K   109G    20K  /export
    rootpool/export/home                     18K   109G    18K  /export/home
    rootpool/swap                             4G   113G    16K  -


    -bash-3.00# lustatus
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    yes       no     -         
    20090721mao                yes      no     no        yes    -         



<span class="c1">## patch the second BE with latest patches (luupgrade)</span>

    <span class="c1">## patch set copied locally:</span>
    -bash-3.00# ls -lad /var/tmp/10_Recommended
    drwxr-xr-x <span class="m">157</span> root     root         <span class="m">161</span> Jul <span class="m">17</span> 12:13 /var/tmp/10_Recommended

    <span class="c1">## Usage:</span>
    luupgrade  - Add Patches:     luupgrade -t -n BE_name <span class="o">[</span> -l error_log <span class="o">]</span> <span class="o">[</span> -o outfile <span class="o">]</span> <span class="o">[</span> -N <span class="o">]</span> <span class="o">[</span> -X <span class="o">]</span> -s source_patches_path 
        <span class="o">[</span> -O patchadd_options <span class="o">]</span> <span class="o">[</span> patchname <span class="o">[</span> patchname... <span class="o">]</span> <span class="o">]</span>

<span class="c1">## Patch:</span>
    -bash-3.00# luupgrade -t -n 20090721mao -s /var/tmp/10_Recommended 

    Validating the contents of the media &lt;/var/tmp/10_Recommended&gt;.
    The media contains <span class="m">155</span> software patches that can be added.
    All <span class="m">155</span> patches will be added because you did not specify any specific patches to add.
    Mounting the BE &lt;20090721mao&gt;.
    Adding patches to the BE &lt;20090721mao&gt;.
    Validating patches...

    Loading patches installed on the system...

    Done!

    Loading patches requested to install.

    Version of package SUNWpfb from directory SUNWpfb.u in patch 118712-23 differs from the package installed on the system.
    Version of package SUNWpfb from directory SUNWpfb.us in patch 118712-23 differs from the package installed on the system.
    Version of package SUNWced from directory SUNWced.us in patch 118777-14 differs from the package installed on the system.
    Version of package SUNWced from directory SUNWced.u in patch 118777-14 differs from the package installed on the system.
    Version of package SUNWkvm from directory SUNWkvm.u in patch 118833-36 differs from the package installed on the system.

    ...
    ...


    Requested patch 140860-01 is already installed on the system.
    Requested patch 140899-01 is already installed on the system.
    Requested patch 141016-01 is already installed on the system.

    The following requested patches <span class="k">do</span> not update any packages installed on the system
    No Packages from patch 139943-01 are installed on the system.

    Checking patches that you specified <span class="k">for</span> installation.

    Done!


    The following requested patches will not be installed because
    they have been made obsolete by other patches already
    installed on the system or by patches you have specified <span class="k">for</span> installation.

               <span class="m">0</span> All packages from patch 118731-01 are patched by higher revision patches.

               <span class="m">1</span> All packages from patch 122660-10 are patched by higher revision patches.

               <span class="m">2</span> All packages from patch 124204-04 are patched by higher revision patches.


    The following requested patches will not be installed because
    the packages they patch are not installed on this system.

               <span class="m">0</span> No Packages from patch 121975-01 are installed on the system.

               <span class="m">1</span> No Packages from patch 118667-19 are installed on the system.

    ...
    ...



    The following requested patches will not be installed because
    at least one required patch is not installed on this system.

               <span class="m">0</span> For patch 120410-31, required patch 121975-01 will not be installed because it updates no packages on this system.

    Approved patches will be installed in this order:

    118666-20 118777-14 118959-04 119059-47 119090-32 119213-19 119254-66 119757-15 
    119783-11 120272-24 122261-02 122911-16 123893-15 125555-05 125952-19 137080-03 
    138322-03 138822-04 138874-03 139604-05 139606-02 139608-04 139966-02 139969-02 
    140074-08 140171-02 140386-04 140391-03 140397-08 140917-01 140921-01 141020-01 
    141414-02 141719-01 141733-03 141742-02 141765-01 141778-02 


    Checking installed patches...
    Verifying sufficient filesystem capacity <span class="o">(</span>dry run method<span class="o">)</span>...
    Installing patch packages...



<span class="c1">## While patching:</span>

    -bash-3.00# ps -eaf <span class="p">|</span>grep luu
    root <span class="m">16270</span> <span class="m">15416</span>   <span class="m">0</span> 14:33:12 pts/2       0:00 grep luu
    root  <span class="m">2434</span> <span class="m">28299</span>   <span class="m">0</span> 14:30:50 pts/1       0:01 /bin/ksh /usr/sbin/luupgrade -t -n 20090721mao -s /var/tmp/10_Recommended

    -bash-3.00# lustatus
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    yes       no     -         
    20090721mao                yes      no     no        yes    -         

    -bash-3.00# ptree <span class="m">2434</span> 
    <span class="m">415</span>   /opt/quest/sbin/sshd
      <span class="m">28296</span> /opt/quest/sbin/sshd -R
        <span class="m">28299</span> -bash
          <span class="m">2434</span>  /bin/ksh /usr/sbin/luupgrade -t -n 20090721mao -s /var/tmp/10_Recom
            <span class="m">2867</span>  /usr/sbin/patchadd -R /a -M /var/tmp/10_Recommended 118666-20 118
              <span class="m">2871</span>  /bin/ksh -hp /usr/lib/patch/patchadd -R /a -M /var/tmp/10_Recom
                <span class="m">8794</span>  pkgadd -O patchPkgInstall -O nozones -O enable-hollow-package
                  <span class="m">8797</span>  /usr/sadm/install/bin/pkginstall -O patchPkgInstall -S -M -
                    <span class="m">29416</span> /sbin/sh /a/var/sadm/pkg/SUNWj5dmo/install/postinstall
                      <span class="m">29563</span> rm -r /a/var/sadm/pkg/SUNWj5dmo/save/SUNWj5dmo

<span class="c1">## back to output:</span>


    Patch 118666-20 has been successfully installed.
    See /a/var/sadm/patch/118666-20/log <span class="k">for</span> details

    Patch packages installed:
      SUNWj5cfg
      SUNWj5dev
      SUNWj5dmo
      SUNWj5man
      SUNWj5rt

    Checking installed patches...
    Executing prepatch script...
    Verifying sufficient filesystem capacity <span class="o">(</span>dry run method<span class="o">)</span>...
    Installing patch packages...

    ...
    ...
    Checking installed patches...
    Verifying sufficient filesystem capacity <span class="o">(</span>dry run method<span class="o">)</span>...
    Installing patch packages...

    Patch 141765-01 has been successfully installed.
    See /a/var/sadm/patch/141765-01/log <span class="k">for</span> details

    Patch packages installed:
      SUNWdtrp

    Checking installed patches...
    Verifying sufficient filesystem capacity <span class="o">(</span>dry run method<span class="o">)</span>...
    Installing patch packages...

    Patch 141778-02 has been successfully installed.
    See /a/var/sadm/patch/141778-02/log <span class="k">for</span> details

    Patch packages installed:
      SUNWcakr
      SUNWkvm
      SUNWldomr
      SUNWldomu

    Unmounting the BE &lt;20090721mao&gt;.
    The patch add to the BE &lt;20090721mao&gt; failed <span class="o">(</span>with result code &lt;1&gt;<span class="o">)</span>. &lt;--------- ??????????

<span class="c1">## Everything looks fine, no idea what generated the failure message</span>
<span class="c1">## Mike C thinks that the patches against uninstalled packages, etc. would generate a return code sufficient to </span>
<span class="c1">## result in the overall &#39;error&#39; warning.</span>

<span class="c1">## TODO:   validate this by patching with a single &#39;good&#39; patch, then with a unneeded patch, and compare</span>
<span class="c1">## luupgrade exit values.</span>
    -bash-3.00# lustatus 
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    yes       no     -         
    20090721mao                yes      no     no        yes    -         


<span class="c1">## Find the file changes since LU creation (the patch activity)</span>
    -bash-3.00# lucompare -o /var/tmp/20090721mao_patch_changes 20090721mao
    Determining the configuration of 20090721mao ...



    Processing Global Zone
    Comparing / ...

    Compare <span class="nb">complete</span> <span class="k">for</span> /.
    Comparing /var ...


    Compare <span class="nb">complete</span> <span class="k">for</span> /var.

<span class="c1">## The ouput file contains all of the changes, including the changetype:</span>

    <span class="k">for</span> example:
     Sizes differ                                                                                   
     <span class="m">01</span> &lt; /usr/postgres/8.3/doc/html/functions-admin.html:root:bin:1:33060:REGFIL:27559:
     <span class="m">02</span> &gt; /usr/postgres/8.3/doc/html/functions-admin.html:root:bin:1:33060:REGFIL:27573:


     Checksums differ                                                                        
     <span class="m">01</span> &lt; /usr/postgres/8.3/doc/html/view-pg-timezone-abbrevs.html:root:bin:1:33060:REGFIL:3617:534836224:
     <span class="m">02</span> &gt; /usr/postgres/8.3/doc/html/view-pg-timezone-abbrevs.html:root:bin:1:33060:REGFIL:3617:2668812025:


     Symbolic links are to different files
     Symbolic links are to different files
     <span class="m">01</span> &lt; /usr/java:root:other:1:41471:SYMLINK:15:
     <span class="m">02</span> &gt; /usr/java:root:other:1:41471:SYMLINK:15:


    etc.


<span class="c1">## Now that we have patched, let&#39;s test mounting the alt BE file system to the live BE, and update a file:</span>

<span class="c1">## from lumount man page:</span>
     The lumount and luumount commands <span class="nb">enable</span>  you  to  mount  or
     unmount  all of the file systems in a boot environment <span class="o">(</span>BE<span class="o">)</span>.
     This allows you to inspect or modify the files in a BE <span class="k">while</span>
     that  BE  is not active. By default, lumount mounts the file
     systems on a mount point of the  form  /.alt.BE_name,  where
     BE_name  is  the name of the BE whose file systems are being
     mounted. See NOTES.

<span class="c1">## So, run it w/o options (except the BE name), and let it do it&#39;s thing</span>
    -bash-3.00# sudo lumount 20090721mao
    /.alt.20090721mao

    <span class="c1"># it very kindly returns the mount point.  =]</span>

    <span class="c1">## df:</span>
                           134G   2.4G   109G     3%    /.alt.20090721mao
    /export                109G    20K   109G     1%    /.alt.20090721mao/export
    /export/home           109G    18K   109G     1%    /.alt.20090721mao/export/home
    /rootpool              109G    94K   109G     1%    /.alt.20090721mao/rootpool
    rootpool/ROOT/20090721mao/var
                           134G   4.4G   109G     4%    /.alt.20090721mao/var
    swap                    12G     0K    12G     0%    /.alt.20090721mao/var/run
    swap                    12G     0K    12G     0%    /.alt.20090721mao/tmp


<span class="c1">## mod a file:</span>
    -bash-3.00# <span class="nb">cd</span> /.alt.20090721mao/etc

    -bash-3.00# cat /.alt.20090721mao/etc/motd 
    Sun Microsystems Inc.   SunOS 5.10      Generic January 2005
    <span class="c1"># mike o rules!                             &lt;---------------   this is not from standard jumpstart  =]</span>


<span class="c1">## now that we have manipulated the BE, unmount it:</span>

    -bash-3.00# luumount 20090721mao

<span class="c1">## Activate the alternate boot environment to be active next reboot:</span>

    <span class="c1">## From luactivate man page:</span>
     The luactivate command, with no arguments, displays the name
     of  the  boot  environment <span class="o">(</span>BE<span class="o">)</span> that will be active upon the
     next reboot of the system. When an argument <span class="o">(</span>a BE<span class="o">)</span> is speci-
     fied, luactivate activates the specified BE.

    -bash-3.00# lustatus    
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    yes       no     -         
    20090721mao                yes      no     no        yes    -         

    -bash-3.00# luactivate
    zfsroot

    -bash-3.00# luactivate 20090721mao
    A Live Upgrade Sync operation will be performed on startup of boot environment &lt;20090721mao&gt;.


    **********************************************************************

    The target boot environment has been activated. It will be used when you 
    reboot. NOTE: You MUST NOT USE the reboot, halt, or uadmin commands. You 
    MUST USE either the init or the shutdown <span class="nb">command</span> when you reboot. If you 
    <span class="k">do</span> not use either init or shutdown, the system will not boot using the 
    target BE.

    **********************************************************************

    In <span class="k">case</span> of a failure <span class="k">while</span> booting to the target BE, the following process 
    needs to be followed to fallback to the currently working boot environment:

    1. Enter the PROM monitor <span class="o">(</span>ok prompt<span class="o">)</span>.

    2. Boot the machine to Single User mode using a different boot device 
    <span class="o">(</span>like the Solaris Install CD or Network<span class="o">)</span>. Examples:

         At the PROM monitor <span class="o">(</span>ok prompt<span class="o">)</span>:
         For boot to Solaris CD:  boot cdrom -s
         For boot to network:     boot net -s

    3. Mount the Current boot environment root slice to some directory <span class="o">(</span>like 
    /mnt<span class="o">)</span>. You can use the following <span class="nb">command</span> to mount:

         mount -Fzfs /dev/dsk/c1t0d0s0 /mnt

    4. Run &lt;luactivate&gt; utility with out any arguments from the current boot 
    environment root slice, as shown below:

         /mnt/sbin/luactivate

    5. luactivate, activates the previous working boot environment and 
    indicates the result.

    6. Exit Single User mode and reboot the machine.

    **********************************************************************

    Modifying boot archive service

    Activation of boot environment &lt;20090721mao&gt; successful.




    <span class="c1">## status:</span>
    -bash-3.00# lustatus
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      yes    no        no     -         
    20090721mao                yes      no     yes       no     -         
    -bash-3.00# luactivate
    20090721mao


<span class="c1">## Now, reboot.    Wait wait - init or shutdown...    =]</span>

    -bash-3.00# who am i 
    root       console      Jul <span class="m">15</span> 12:37
    -bash-3.00# hostname
    pststapp01
    -bash-3.00# shutdown -i6 -g0 -y

    ...
    SPARC Enterprise T5220, No Keyboard
    Copyright <span class="m">2009</span> Sun Microsystems, Inc.  All rights reserved.
    OpenBoot 4.30.2.b, <span class="m">32640</span> MB memory available, Serial <span class="c1">#85380300.</span>
    Ethernet address 0:21:28:16:cc:cc, Host ID: 8516cccc.



    Boot device: /pci@0/pci@0/pci@2/scsi@0/disk@0,0:a  File and args: 
    SunOS Release 5.10 Version Generic_141414-02 64-bit
    Copyright 1983-2009 Sun Microsystems, Inc.  All rights reserved.
    Use is subject to license terms.

    ...



    Configuring devices.
    Loading smf<span class="o">(</span>5<span class="o">)</span> service descriptions: 12/12
    Reading ZFS config: <span class="k">done</span>.
    Mounting ZFS filesystems: <span class="o">(</span>8/8<span class="o">)</span>
    pststapp01 console login: 


    <span class="o">[</span>08:47:19 slurm<span class="o">]</span> ~ $ ssh 10.200.19.100 -l root
    Last login: Tue Jul <span class="m">21</span> 10:42:56 <span class="m">2009</span> from slurm.thechildr
    Sun Microsystems Inc.   SunOS 5.10      Generic January 2005
    <span class="c1"># mike o rules!                       &lt;--------------------------    file modification from inactive BE, now applied.</span>


    -bash-3.00# lustatus
    Boot Environment           Is       Active Active    Can    Copy      
    Name                       Complete Now    On Reboot Delete Status    
    -------------------------- -------- ------ --------- ------ ----------
    zfsroot                    yes      no     no        yes    -         
    20090721mao                yes      yes    yes       no     -         
    -bash-3.00# luactivate
    20090721mao

    <span class="c1">## patch levels:</span>
    <span class="c1">## (just  checking a couple of random levels from those that would have been applied):</span>

    -bash-3.00# showrev -p <span class="p">|</span>grep <span class="s1">&#39;Patch: 139608&#39;</span>  
    Patch: 139608-02 Obsoletes: 120220-01, 120222-31, 125740-01, 136786-01 Requires:  Incompatibles:  Packages: SUNWemlxu, SUNWemlxs
    Patch: 139608-04 Obsoletes: 120220-01, 120222-31, 125740-01, 136786-01 Requires:  Incompatibles:  Packages: SUNWemlxu, SUNWemlxs

    -bash-3.00# uname -a 
    SunOS pststapp01 5.10 Generic_141414-02 sun4v sparc SUNW,SPARC-Enterprise-T5220
    -bash-3.00# cat /etc/release 
                       Solaris <span class="m">10</span> 5/09 s10s_u7wos_08 SPARC
           Copyright <span class="m">2009</span> Sun Microsystems, Inc.  All Rights Reserved.
                        Use is subject to license terms.
                             Assembled <span class="m">30</span> March 2009

    <span class="c1">##</span>
    <span class="c1">## Here&#39;s the zfs list output, with observations:</span>


    -bash-3.00# zfs list
    NAME                                        USED  AVAIL  REFER  MOUNTPOINT
    rootpool                                   25.3G   109G    94K  /rootpool
    rootpool/ROOT                              17.3G   109G    18K  legacy
    rootpool/ROOT/20090721mao                  7.25G   109G  2.19G  /
    rootpool/ROOT/20090721mao@20090714a        38.5K      -  2.43G  -       &lt;----- these snapshots were all from the orig BE
    rootpool/ROOT/20090721mao@20090714b          38K      -  2.43G  -       &lt;---   ...
    rootpool/ROOT/20090721mao@20090721mao      4.69M      -  2.43G  -       &lt;---   ...
    rootpool/ROOT/20090721mao/var              4.44G   109G  4.39G  /var
    rootpool/ROOT/20090721mao/var@20090714b    10.4M      -   283M  -       &lt;---   ...
    rootpool/ROOT/20090721mao/var@20090721mao  29.7M      -  4.06G  -       &lt;---   ...
    rootpool/ROOT/zfsroot                      10.0G   119G  2.43G  /       &lt;------ original BE
    rootpool/ROOT/zfsroot/var                  33.6M  9.97G  4.07G  /var    &lt;------ original BE
    rootpool/dump                              4.00G   109G  4.00G  -
    rootpool/export                              38K   109G    20K  /export
    rootpool/export/home                         18K   109G    18K  /export/home
    rootpool/swap                                 4G   113G    16K  -



    <span class="c1">## Everything looks good.</span>

    <span class="c1">## Now, testing the new PROM boot -Z option to test boot from BE choice menu:</span>

    <span class="c1">## The lucreate command added the new BE to the menu.1st file automagically:</span>

    -bash-3.00# <span class="nb">pwd</span>
    /rootpool/boot

    -bash-3.00# cat  menu.lst 
    title zfsroot
    bootfs rootpool/ROOT/zfsroot
    title 20090721mao
    bootfs rootpool/ROOT/20090721mao

    <span class="c1">## so, shut it down to ok prompt ( *not* changing the BE with luactivate) :</span>
    -bash-3.00# who am i <span class="p">;</span> hostname
    root       console      Jul <span class="m">22</span> 09:02
    pststapp01

    -bash-3.00# shutdown -i <span class="m">0</span> -g0 -y

    <span class="c1">## </span>

    SPARC Enterprise T5220, No Keyboard
    Copyright <span class="m">2009</span> Sun Microsystems, Inc.  All rights reserved.
    OpenBoot 4.30.2.b, <span class="m">32640</span> MB memory available, Serial <span class="c1">#85380300.</span>
    Ethernet address 0:21:28:16:cc:cc, Host ID: 8516cccc.



    <span class="o">{</span>0<span class="o">}</span> ok boot -L
    Boot device: /pci@0/pci@0/pci@2/scsi@0/disk@0,0:a  File and args: -L
    <span class="m">1</span> zfsroot
    <span class="m">2</span> 20090721mao
    Select environment to boot: <span class="o">[</span> <span class="m">1</span> - <span class="m">2</span> <span class="o">]</span>: 1

    To boot the selected entry, invoke:
    boot <span class="o">[</span>&lt;root-device&gt;<span class="o">]</span> -Z rootpool/ROOT/zfsroot

    Program terminated
    <span class="o">{</span>0<span class="o">}</span> ok boot  -Z rootpool/ROOT/zfsroot


    Boot device: /pci@0/pci@0/pci@2/scsi@0/disk@0,0:a  File and args: -Z rootpool/ROOT/zfsroot
    krtld: Ignoring invalid kernel option -Z.
    krtld: Unused kernel arguments: <span class="sb">`</span>rootpool/ROOT/zfsroot<span class="s1">&#39;.</span>

<span class="s1">    # From:</span>
<span class="s1">    # http://www.solarisinternals.com/wiki/index.php/ZFS_Troubleshooting_Guide</span>
<span class="s1">        &quot;   </span>
<span class="s1">         ZFS Boot Error Messages</span>

<span class="s1">            * CR 2164779 - Ignore the following krtld messages from the boot -Z command. They are harmless: </span>

<span class="s1">          krtld: Ignoring invalid kernel option -Z.</span>
<span class="s1">          krtld: Unused kernel arguments: `rpool/ROOT/zfs1008BE&#39;</span>.

        <span class="s2">&quot;</span>
<span class="s2">    -bash-3.00# luactivate</span>
<span class="s2">    zfsroot</span>
<span class="s2">    -bash-3.00# lustatus</span>
<span class="s2">    Boot Environment           Is       Active Active    Can    Copy      </span>
<span class="s2">    Name                       Complete Now    On Reboot Delete Status    </span>
<span class="s2">    -------------------------- -------- ------ --------- ------ ----------</span>
<span class="s2">    zfsroot                    yes      yes    yes       no     -         </span>
<span class="s2">    20090721mao                yes      no     no        yes    -   </span>

<span class="s2">    ## And, obviously back in the original BE:</span>

<span class="s2">    # back to the boring default.</span>
<span class="s2">    -bash-3.00# cat /etc/motd</span>
<span class="s2">    Sun Microsystems Inc.   SunOS 5.10      Generic January 2005</span>

<span class="s2">    # 20090721mao patched BE is Generic_141414-02</span>
<span class="s2">    -bash-3.00# uname -a</span>
<span class="s2">    SunOS pststapp01 5.10 Generic_139555-08 sun4v sparc SUNW,SPARC-Enterprise-T5220   </span>

<span class="s2">    # 20090721mao patched BE was 139608-04</span>
<span class="s2">    -bash-3.00# showrev -p |grep &#39;Patch: 139608&#39;</span>
<span class="s2">    Patch: 139608-02 Obsoletes: 120220-01, 120222-31, 125740-01, 136786-01 Requires:  Incompatibles:  Packages: SUNWemlxu, SUNWemlxs</span>

<span class="s2">    NAME                                        USED  AVAIL  REFER  MOUNTPOINT</span>
<span class="s2">    rootpool                                   25.3G   109G    94K  /rootpool</span>
<span class="s2">    rootpool/ROOT                              17.3G   109G    18K  legacy</span>
<span class="s2">    rootpool/ROOT/20090721mao                  7.25G   109G  2.19G  /</span>
<span class="s2">    rootpool/ROOT/20090721mao@20090714a        38.5K      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/20090721mao@20090714b          38K      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/20090721mao@20090721mao      4.69M      -  2.43G  -</span>
<span class="s2">    rootpool/ROOT/20090721mao/var              4.44G   109G  4.39G  /var</span>
<span class="s2">    rootpool/ROOT/20090721mao/var@20090714b    10.4M      -   283M  -</span>
<span class="s2">    rootpool/ROOT/20090721mao/var@20090721mao  29.7M      -  4.06G  -</span>
<span class="s2">    rootpool/ROOT/zfsroot                      10.0G   119G  2.43G  /</span>
<span class="s2">    rootpool/ROOT/zfsroot/var                  33.9M  9.97G  4.07G  /var</span>
<span class="s2">    rootpool/dump                              4.00G   109G  4.00G  -</span>
<span class="s2">    rootpool/export                              38K   109G    20K  /export</span>
<span class="s2">    rootpool/export/home                         18K   109G    18K  /export/home</span>
<span class="s2">    rootpool/swap                                 4G   113G    16K  -</span>
</pre></div>


<h1>ZFS Adding Swap device</h1>
<div class="highlight"><pre><span></span><span class="o">[</span>13:10:52 dbtst01<span class="o">]</span> ~ $ sudo zfs create -b <span class="m">8192</span> -V 4096m rootpool/swap1     

<span class="o">[</span>13:11:10 dbtst01<span class="o">]</span> ~ $ sudo swap -a /dev/zvol/dsk/rootpool/swap1
</pre></div>


<h1>ZFS Boot into Single User</h1>
<div class="highlight"><pre><span></span>SINGLE USER MODE

<span class="c1"># zpool list</span>
no pools available
<span class="c1"># zfs list</span>
no datasets available

<span class="c1"># zpool import -R /mnt -a</span>

<span class="c1"># zfs list -t filesystem</span>
NAME                          USED  AVAIL  REFER  MOUNTPOINT
rootpool                     15.0G   119G    94K  /mnt/rootpool
rootpool/ROOT                6.93G   119G    18K  legacy
rootpool/ROOT/initialBE      6.93G   119G  3.71G  /mnt
rootpool/ROOT/initialBE/var  2.91G  12.1G  2.86G  /mnt/var
rootpool/export               722K  10.0G    20K  /mnt/export
rootpool/export/home          670K  10.0G   233K  /mnt/export/home




<span class="c1">## now that we have the zpool imported, we can mount the ZFS filesystems that we need:</span>

<span class="c1"># zfs mount rootpool/ROOT/initialBE</span>

<span class="c1"># ls -la /mnt</span>
total 140
drwxr-xr-x  <span class="m">27</span> root     root          <span class="m">32</span> Nov <span class="m">13</span> 12:34 .
drwxr-xr-x  <span class="m">19</span> root     root         <span class="m">512</span> Mar <span class="m">30</span>  <span class="m">2009</span> ..
drwxr-xr-x   <span class="m">2</span> root     sys            <span class="m">2</span> Oct <span class="m">24</span> 16:23 .@localstatedirroot@
drwxr-xr-x   <span class="m">2</span> root     sys            <span class="m">2</span> Oct <span class="m">24</span> 16:23 .@sysconfdirroot@
-rw-------   <span class="m">1</span> root     root          <span class="m">24</span> Oct <span class="m">25</span> 16:19 .bash_history
-rw-r--r--   <span class="m">1</span> root     root          <span class="m">40</span> Aug <span class="m">19</span> 09:54 .forward
-rwxr--r--   <span class="m">1</span> root     root          <span class="m">46</span> Nov  <span class="m">4</span> 12:26 .osuuid
drwx------   <span class="m">2</span> root     root           <span class="m">3</span> Oct <span class="m">24</span> 16:01 .ssh
drwx------   <span class="m">3</span> root     root           <span class="m">3</span> Oct <span class="m">24</span> 16:23 .sunw
lrwxrwxrwx   <span class="m">1</span> root     root           <span class="m">9</span> Oct <span class="m">24</span> 15:54 bin -&gt; ./usr/bin
drwxr-xr-x   <span class="m">3</span> root     sys            <span class="m">3</span> Oct <span class="m">24</span> 15:55 boot
drwxr-xr-x  <span class="m">21</span> root     sys          <span class="m">294</span> Nov <span class="m">13</span> 12:15 dev
drwxr-xr-x  <span class="m">12</span> root     sys           <span class="m">12</span> Nov <span class="m">17</span> 08:24 devices
drwxr-xr-x  <span class="m">69</span> root     sys          <span class="m">225</span> Nov <span class="m">13</span> 12:34 etc
drwxr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Nov  <span class="m">5</span> 14:32 <span class="nb">export</span>
dr-xr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 15:56 home
drwxr-xr-x  <span class="m">15</span> root     sys           <span class="m">15</span> Oct <span class="m">24</span> 15:57 kernel
drwxr-xr-x   <span class="m">7</span> root     bin          <span class="m">243</span> Oct <span class="m">24</span> 15:58 lib
drwxr-xr-x   <span class="m">2</span> root     sys            <span class="m">2</span> Oct <span class="m">24</span> 15:54 mnt
dr-xr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 16:23 net
-rw-r--r--   <span class="m">1</span> root     root           <span class="m">0</span> Oct <span class="m">24</span> 16:01 noautoshutdown
drwxr-xr-x  <span class="m">36</span> root     sys           <span class="m">36</span> Nov <span class="m">13</span> 08:11 opt
drwxr-xr-x  <span class="m">40</span> root     sys           <span class="m">47</span> Oct <span class="m">24</span> 15:55 platform
dr-xr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 15:54 proc
drwxr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 15:54 rootpool
drwxr-xr-x   <span class="m">2</span> root     sys           <span class="m">66</span> Nov  <span class="m">4</span> 12:49 sbin
drwxr-xr-x   <span class="m">4</span> root     root           <span class="m">4</span> Oct <span class="m">24</span> 15:54 system
drwxr-xr-x  <span class="m">10</span> root     root          <span class="m">10</span> Nov  <span class="m">5</span> 13:21 tch
drwxrwxrwt   <span class="m">2</span> root     sys            <span class="m">2</span> Nov <span class="m">17</span> 08:25 tmp
drwxr-xr-x  <span class="m">35</span> root     sys           <span class="m">49</span> Nov <span class="m">13</span> 08:11 usr
drwxr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 15:54 var
drwxr-xr-x   <span class="m">2</span> root     root           <span class="m">2</span> Oct <span class="m">24</span> 16:23 vol


<span class="c1">## Now in this case, we wish to revert to pre- &#39;stsmboot -e&#39; modifications.</span>

<span class="c1">## so, I exported my TERM to xterms, and edited /mnt/kernel/drv/fp.conf, removing the </span>
<span class="c1">## damage done by the stmsboot cmd - </span>

from:
disable-sata-mpxio<span class="o">=</span><span class="s2">&quot;no&quot;</span><span class="p">;</span>

to:
<span class="c1">#disable-sata-mpxio=&quot;no&quot;;</span>

and rebooted.
</pre></div>
    </div><!-- /.entry-content -->

  </article>
</section>
        <section id="extras" class="body">
                <div class="blogroll">
                        <h2>blogroll</h2>
                        <ul>
                            <li><a href="http://getpelican.com/">Pelican</a></li>
                            <li><a href="http://python.org/">Python.org</a></li>
                            <li><a href="http://jinja.pocoo.org/">Jinja2</a></li>
                        </ul>
                </div><!-- /.blogroll -->
                <div class="social">
                        <h2>social</h2>
                        <ul>

                            <li><a href="https://github.com/craigriley39">Github</a></li>
                        </ul>
                </div><!-- /.social -->
        </section><!-- /#extras -->

        <footer id="contentinfo" class="body">
                <p>Powered by <a href="http://getpelican.com/">Pelican</a>. Theme <a href="https://github.com/blueicefield/pelican-blueidea/">blueidea</a>, inspired by the default theme.</p>
        </footer><!-- /#contentinfo -->

</body>
</html>